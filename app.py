import streamlit as st
import google.generativeai as genai
from github import Github
import json

# ================= 1. åŸºç¡€é…ç½® =================
st.set_page_config(page_title="æˆ‘çš„ç§äººAIæŒ‡æŒ¥å°", page_icon="ğŸ§ ", layout="wide")

api_key = st.secrets.get("GEMINI_API_KEY")
github_token = st.secrets.get("GITHUB_TOKEN")
repo_name = st.secrets.get("REPO_NAME")

if not api_key or not github_token or not repo_name:
    st.error("âš ï¸ ç¼ºå°‘å¯†é’¥ï¼è¯·æ£€æŸ¥ Streamlit Secrets")
    st.stop()

genai.configure(api_key=api_key)

# ================= 2. GitHub æ•°æ®è¯»å†™ =================
def get_roles():
    try:
        g = Github(github_token)
        repo = g.get_repo(repo_name)
        try:
            contents = repo.get_contents("roles.json")
            return json.loads(contents.decoded_content.decode()), contents.sha
        except:
            return {}, None
    except:
        return {}, None

def save_roles(roles, sha):
    try:
        g = Github(github_token)
        repo = g.get_repo(repo_name)
        content_str = json.dumps(roles, indent=2, ensure_ascii=False)
        if sha:
            repo.update_file("roles.json", "Update", content_str, sha)
        else:
            repo.create_file("roles.json", "Init", content_str)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜å¤±è´¥: {e}")
        return False

# ================= 3. æ ¸å¿ƒï¼šè‡ªåŠ¨è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ =================
@st.cache_data(ttl=3600) # ç¼“å­˜1å°æ—¶ï¼Œé¿å…æ¯æ¬¡éƒ½è¯·æ±‚
def get_available_models():
    """ç›´æ¥é—® Google åˆ°åº•æœ‰å“ªäº›æ¨¡å‹å¯ç”¨"""
    try:
        model_list = []
        for m in genai.list_models():
            # åªç­›é€‰æ”¯æŒç”Ÿæˆçš„ Gemini æ¨¡å‹
            if 'generateContent' in m.supported_generation_methods:
                # è¿‡æ»¤æ‰è€æ—§æ¨¡å‹ï¼Œåªç•™ Gemini ç³»åˆ—
                if "gemini" in m.name:
                    # å»æ‰ 'models/' å‰ç¼€ï¼Œåªç•™åå­—
                    clean_name = m.name.replace("models/", "")
                    model_list.append(clean_name)
        # æŠŠæœ€æ–°çš„ 3.0 æ’åœ¨å‰é¢ (å€’åºæ’åˆ—)
        model_list.sort(reverse=True)
        return model_list
    except Exception as e:
        st.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ä¿åº•åˆ—è¡¨
        return ["gemini-2.0-flash-exp", "gemini-1.5-pro"]

# ================= 4. é¡µé¢é€»è¾‘ =================
st.title("ğŸ¤– æˆ‘çš„ç§äºº AI åŠ©ç† (Gemini 3.0 Ready)")

roles_data, file_sha = get_roles()
available_models = get_available_models() # è·å–çœŸå®æ¨¡å‹åˆ—è¡¨

tab1, tab2 = st.tabs(["ğŸ’¬ å¼€å§‹å¯¹è¯", "âš™ï¸ è§’è‰²ç®¡ç†"])

with tab1:
    if not roles_data:
        st.info("ğŸ‘‹ è¯·å…ˆå»ã€è§’è‰²ç®¡ç†ã€‘æ–°å»ºä¸€ä¸ªè§’è‰²ï¼")
    else:
        with st.sidebar:
            st.header("ğŸ§  å¤§è„‘è®¾ç½®")
            
            # === è¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ ===
            # ä¸‹æ‹‰æ¡†ç›´æ¥ä½¿ç”¨ä» Google è·å–çš„çœŸå®åˆ—è¡¨
            st.success(f"å·²æ£€æµ‹åˆ° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
            model_version = st.selectbox(
                "é€‰æ‹©æ¨¡å‹ (å·²è‡ªåŠ¨åŒæ­¥æœ€æ–°ç‰ˆ)", 
                available_models,
                index=0 # é»˜è®¤é€‰ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€æ–°çš„ï¼‰
            )
            
            if st.button("ğŸ§¹ æ¸…ç©ºèŠå¤©"):
                st.session_state.messages = []
                st.rerun()

        selected_role = st.selectbox("ğŸ‘‰ é€‰æ‹©è§’è‰²ï¼š", list(roles_data.keys()))
        current_prompt = roles_data[selected_role]
        
        with st.expander(f"æŸ¥çœ‹è®¾å®š"):
            st.info(current_prompt)

        if "messages" not in st.session_state:
            st.session_state.messages = []

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if user_input := st.chat_input("è¾“å…¥..."):
            with st.chat_message("user"):
                st.markdown(user_input)
            st.session_state.messages.append({"role": "user", "content": user_input})

            try:
                history = [{"role": ("user" if m["role"]=="user" else "model"), "parts": [m["content"]]} for m in st.session_state.messages[:-1]]
                model = genai.GenerativeModel(model_version)
                chat = model.start_chat(history=history)
                response = chat.send_message(f"ã€ç³»ç»ŸæŒ‡ä»¤ã€‘ï¼š{current_prompt}\n\nã€ç”¨æˆ·ã€‘ï¼š{user_input}")
                
                with st.chat_message("assistant"):
                    st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"å‡ºé”™: {e}")

with tab2:
    st.header("ğŸ› ï¸ è§’è‰²åº“")
    action = st.radio("æ“ä½œ", ["â• æ–°å»º", "âœï¸ ç¼–è¾‘"], horizontal=True)
    st.divider()

    if action == "â• æ–°å»º":
        name = st.text_input("æ–°è§’è‰²å")
        prompt = st.text_area("è®¾å®š", height=200)
        if st.button("ä¿å­˜", type="primary"):
            if name and prompt:
                roles_data[name] = prompt
                if save_roles(roles_data, file_sha):
                    st.success("æˆåŠŸ")
                    st.rerun()
    else:
        if roles_data:
            target = st.selectbox("ç¼–è¾‘å¯¹è±¡", list(roles_data.keys()))
            old_prompt = roles_data[target]
            col1, col2 = st.columns(2)
            with col1: new_name = st.text_input("åç§°", value=target)
            with col2: new_prompt = st.text_area("è®¾å®š", value=old_prompt, height=150)
            
            c1, c2 = st.columns([1,4])
            with c1:
                if st.button("ğŸ’¾ ä¿å­˜"):
                    if new_name != target: del roles_data[target]
                    roles_data[new_name] = new_prompt
                    save_roles(roles_data, file_sha)
                    st.rerun()
            with c2:
                if st.button("ğŸ—‘ï¸ åˆ é™¤"):
                    del roles_data[target]
                    save_roles(roles_data, file_sha)
                    st.rerun()
